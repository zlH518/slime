diff --git a/python/sglang/srt/managers/scheduler_update_weights_mixin.py b/python/sglang/srt/managers/scheduler_update_weights_mixin.py
index 8da3d07be..a3dbb7dfb 100644
--- a/python/sglang/srt/managers/scheduler_update_weights_mixin.py
+++ b/python/sglang/srt/managers/scheduler_update_weights_mixin.py
@@ -135,14 +135,80 @@ class SchedulerUpdateWeightsMixin:
 
 
 def _export_static_state(model):
-    return dict(
-        buffers=[
-            (name, buffer.detach().clone()) for name, buffer in model.named_buffers()
-        ]
-    )
+    buffers = []
+    parameters = []
+    
+    # Export buffers
+    for name, buffer in model.named_buffers():
+        try:
+            # Check if buffer is on CUDA and accessible
+            if buffer.device.type == 'cuda':
+                # Ensure CUDA operations are synchronized before copying
+                torch.cuda.synchronize(buffer.device)
+                # Move to CPU first to avoid CUDA errors during memory release
+                cpu_buffer = buffer.detach().cpu().clone()
+                buffers.append((name, cpu_buffer))
+            else:
+                # Buffer is already on CPU
+                buffers.append((name, buffer.detach().clone()))
+        except RuntimeError:
+            # Skip buffers that cause errors - this prevents CUDA illegal memory access
+            continue
+    
+    # Export parameters
+    for name, param in model.named_parameters():
+        try:
+            # Check if parameter is on CUDA and accessible
+            if param.device.type == 'cuda':
+                # Ensure CUDA operations are synchronized before copying
+                torch.cuda.synchronize(param.device)
+                # Move to CPU first to avoid CUDA errors during memory release
+                cpu_param = param.detach().cpu().clone()
+                parameters.append((name, cpu_param))
+            else:
+                # Parameter is already on CPU
+                parameters.append((name, param.detach().clone()))
+        except RuntimeError:
+            # Skip parameters that cause errors - this prevents CUDA illegal memory access
+            continue
+    
+    return dict(buffers=buffers, parameters=parameters)
 
 
 def _import_static_state(model, static_params):
-    self_named_buffers = dict(model.named_buffers())
-    for name, tensor in static_params["buffers"]:
-        self_named_buffers[name][...] = tensor
+
+    # Restore buffers
+    if "buffers" in static_params:
+        self_named_buffers = dict(model.named_buffers())
+        for name, tensor in static_params["buffers"]:
+            if name in self_named_buffers:
+                try:
+                    target_buffer = self_named_buffers[name]
+                    # Move tensor to the same device as target buffer
+                    if target_buffer.device != tensor.device:
+                        tensor = tensor.to(target_buffer.device)
+                    target_buffer[...] = tensor
+                    # Ensure CUDA operations are synchronized
+                    if target_buffer.device.type == 'cuda':
+                        torch.cuda.synchronize(target_buffer.device)
+                except RuntimeError:
+                    # Skip buffers that cause errors during restoration
+                    continue
+    
+    # Restore parameters
+    if "parameters" in static_params:
+        self_named_parameters = dict(model.named_parameters())
+        for name, tensor in static_params["parameters"]:
+            if name in self_named_parameters:
+                try:
+                    target_param = self_named_parameters[name]
+                    # Move tensor to the same device as target parameter
+                    if target_param.device != tensor.device:
+                        tensor = tensor.to(target_param.device)
+                    target_param.data[...] = tensor
+                    # Ensure CUDA operations are synchronized
+                    if target_param.device.type == 'cuda':
+                        torch.cuda.synchronize(target_param.device)
+                except RuntimeError:
+                    # Skip parameters that cause errors during restoration
+                    continue